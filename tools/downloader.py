import json
import os
import time
import requests
import re
import subprocess
import sys
from pathlib import Path

def flush_print(msg):
    msg = msg.encode('ascii', 'ignore').decode('ascii')
    print(msg)
    sys.stdout.flush()

MAP_FILE = Path("map.json")
COOKIES_FILE = Path("cookies.json")
COOKIES_NETSCAPE = Path("cookies_netscape.txt")
SETTINGS_FILE = Path("config/settings.json")

def save_cookies_netscape(json_path, netscape_path):
    """Convert Playwright JSON cookies to Netscape format for yt-dlp"""
    if not json_path.exists(): return False
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            cookies = json.load(f)
        
        with open(netscape_path, "w", encoding="utf-8") as f:
            f.write("# Netscape HTTP Cookie File\n")
            f.write("# This file is generated by the script - do not edit\n\n")
            for c in cookies:
                domain = c.get('domain', '')
                # flag: TRUE if domain starts with .
                flag = "TRUE" if domain.startswith('.') else "FALSE"
                path = c.get('path', '/')
                secure = "TRUE" if c.get('secure') else "FALSE"
                # expiration: convert from float to int
                expires = int(c.get('expires', 0))
                if expires == -1 or expires == 0:
                    expires = int(time.time() + 31536000) # 1 year default
                name = c.get('name', '')
                value = c.get('value', '')
                f.write(f"{domain}\t{flag}\t{path}\t{secure}\t{expires}\t{name}\t{value}\n")
        return True
    except Exception as e:
        flush_print(f"   [ERR] Cookie conversion failed: {e}")
        return False

def get_config():
    if SETTINGS_FILE.exists():
        with open(SETTINGS_FILE, "r") as f:
            return json.load(f)
    return {"output_dir": "downloads"}

def get_requests_session():
    session = requests.Session()
    if COOKIES_FILE.exists():
        try:
            with open(COOKIES_FILE, "r", encoding="utf-8") as f:
                cookies = json.load(f)
                for cookie in cookies:
                    session.cookies.set(cookie['name'], cookie['value'], domain=cookie.get('domain', ''))
            flush_print("   [SESSION] Cookies loaded for downloads.")
        except Exception as e:
            flush_print(f"   [WARN] Failed to load session cookies: {e}")
    return session

def sanitize_filename(name):
    # Remove emojis and illegal chars
    clean = re.sub(r'[^\x00-\x7F]+', '', str(name))
    clean = re.sub(r'[<>:"/\\|?*]', '', clean).strip()
    return clean[:100]

def download_file(url, folder, filename, session=None, retries=3):
    path = folder / sanitize_filename(filename)
    if path.exists(): return True
    
    for attempt in range(retries):
        try:
            flush_print(f"      [FILE] Downloading (Attempt {attempt+1}/{retries}): {filename}...")
            caller = session if session else requests
            r = caller.get(url, stream=True, timeout=30)
            r.raise_for_status()
            with open(path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            return True
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(2)
            else:
                flush_print(f"      [ERR] Download failed after {retries} attempts ({filename}): {e}")
                return False
    return False

def save_html(title, body_html, resources_html, filepath):
    if not body_html: body_html = "<p>No description available.</p>"
    template = f"""
<!DOCTYPE html>
<html><head><meta charset="UTF-8"><title>{title}</title>
<style>
    body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; max-width: 950px; margin: 40px auto; padding: 20px; background: #f8fafc; color: #0f172a; }}
    .header {{ border-bottom: 2px solid #6366f1; padding-bottom: 24px; margin-bottom: 32px; }}
    h1 {{ font-size: 2.5rem; font-weight: 800; color: #1e1b4b; margin: 0; }}
    .content {{ background: white; padding: 48px; border-radius: 16px; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1); border: 1px solid #e2e8f0; }}
    img {{ max-width: 100%; height: auto; border-radius: 12px; margin: 28px 0; display: block; box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1); }}
    .resources {{ background: #f0f7ff; padding: 32px; border-radius: 16px; margin-top: 48px; border-left: 8px solid #6366f1; box-shadow: inset 0 2px 4px 0 rgb(0 0 0 / 0.05); }}
    .resources h3 {{ margin-top: 0; color: #1e40af; font-size: 1.5rem; }}
    ul, ol {{ padding-left: 24px; margin-bottom: 20px; }}
    li {{ margin-bottom: 10px; }}
    p {{ margin-bottom: 1.5rem; }}
    a {{ color: #4338ca; text-decoration: none; font-weight: 600; }}
    a:hover {{ text-decoration: underline; color: #3730a3; }}
    code {{ background: #f1f5f9; padding: 2px 6px; border-radius: 4px; font-family: monospace; font-size: 0.9em; color: #e11d48; }}
    blockquote {{ border-left: 4px solid #cbd5e1; padding-left: 20px; margin: 20px 0; color: #64748b; font-style: italic; }}
    hr {{ border: 0; border-top: 1px solid #e2e8f0; margin: 30px 0; }}
</style></head>
<body>
    <div class="header"><h1>{title}</h1></div>
    <div class="content">{body_html}</div>
    <div class="resources"><h3>ðŸ”— Resources & Assets</h3>{resources_html}</div>
</body></html>"""
    with open(filepath, 'w', encoding='utf-8') as f: f.write(template)

def convert_to_html_blocks(desc_data):
    if not desc_data: return ""
    if isinstance(desc_data, str):
        cleaned = desc_data.strip()
        if cleaned.startswith('<'): return cleaned
        if cleaned.startswith("[v2]"): cleaned = cleaned.replace("[v2]", "", 1)
        try: data = json.loads(cleaned)
        except: return f"<p>{cleaned}</p>"
    else: data = desc_data
    if not isinstance(data, list): return f"<p>{str(data)}</p>"
    
    html = ""
    for node in data:
        ntype = node.get('type')
        content = node.get('content', [])
        attrs = node.get('attrs', {})
        
        def process_content(nodes):
            res = ""
            for c in nodes:
                ctype = c.get('type')
                c_content = c.get('content', [])
                if ctype == 'text':
                    t = c.get('text', '')
                    for m in c.get('marks', []):
                        mtype = m.get('type')
                        if mtype == 'bold': t = f"<b>{t}</b>"
                        if mtype == 'italic': t = f"<i>{t}</i>"
                        if mtype == 'link': t = f"<a href='{m['attrs']['href']}' target='_blank'>{t}</a>"
                        if mtype == 'code': t = f"<code>{t}</code>"
                    res += t
                elif ctype == 'hardBreak': res += "<br>"
                elif ctype == 'paragraph': res += f"<p>{process_content(c_content)}</p>"
                elif ctype == 'bulletList': res += f"<ul>{process_content(c_content)}</ul>"
                elif ctype == 'orderedList': res += f"<ol>{process_content(c_content)}</ol>"
                elif ctype == 'listItem': res += f"<li>{process_content(c_content)}</li>"
                elif ctype == 'image':
                    src = c.get('attrs', {}).get('src')
                    if src: res += f"<img src='{src}'>"
            return res

        inner = process_content(content)
        
        if ntype == 'paragraph': html += f"<p>{inner}</p>"
        elif ntype == 'heading': html += f"<h{attrs.get('level', 1)}>{inner}</h{attrs.get('level', 1)}>"
        elif ntype == 'bulletList': html += f"<ul>{inner}</ul>"
        elif ntype == 'orderedList': html += f"<ol>{inner}</ol>"
        elif ntype == 'listItem': html += f"<li>{inner}</li>"
        elif ntype == 'blockquote': html += f"<blockquote>{inner}</blockquote>"
        elif ntype == 'horizontalRule': html += "<hr>"
        elif ntype == 'image':
            src = attrs.get('src') or attrs.get('originalSrc')
            if src: html += f"<img src='{src}'>"
            
    return html

def download_video(url, output_path, retries=3):
    # Support multiple extensions for existing file check
    for ext in ['.mp4', '.mkv', '.webm']:
        if os.path.exists(str(output_path) + ext): return True
    
    for attempt in range(retries):
        try:
            flush_print(f"      [VIDEO] Downloading (Attempt {attempt+1}/{retries}): {url}")
            # Explicit template for output path to ensure extension handling
            cmd = ["yt-dlp", "-f", "bestvideo[ext=mp4]+bestaudio[ext=m4a]/best", "--no-warnings", "-o", f"{str(output_path)}.%(ext)s", url]
            if COOKIES_NETSCAPE.exists():
                cmd.extend(["--cookies", str(COOKIES_NETSCAPE)])
            
            result = subprocess.run(cmd, capture_output=True)
            if result.returncode == 0:
                return True
            else:
                flush_print(f"      [WARN] yt-dlp error: {result.stderr.decode('utf-8', 'ignore')}")
        except Exception as e:
            flush_print(f"      [ERR] Video download exception: {e}")
        
        if attempt < retries - 1:
            time.sleep(10)
            
    return False

def process_node(node, parent_path, session=None):
    title = sanitize_filename(node.get('title', 'Untitled'))
    node_path = parent_path / title
    os.makedirs(node_path, exist_ok=True)
    if node.get('unitType') == 'module':
        flush_print(f"   [SYNC] Content: {title}")
        meta = node.get('metadata', {})
        
        body_html = convert_to_html_blocks(meta.get('desc'))
        
        all_resources = []
        json_atts = meta.get('attachments', [])
        if isinstance(json_atts, str):
            try: json_atts = json.loads(json_atts)
            except: json_atts = []
        for a in json_atts:
            all_resources.append({'name': a.get('title', a.get('file_name', 'File')), 'url': a.get('link', a.get('url', '#'))})
        dom_res = meta.get('resource_links', [])
        for r in dom_res:
            if not any(ex['url'] == r['url'] for ex in all_resources):
                all_resources.append({'name': r.get('name', 'Resource'), 'url': r.get('url')})
        
        res_html = "<ul>"
        if not all_resources: res_html += "<li>No additional files.</li>"
        else:
            # Expanded list of file extensions from user feedback
            exts = ['.json', '.zip', '.pdf', '.png', '.jpg', '.jpeg', '.xlsx', '.csv', '.docx', '.pptx', '.mp3', '.mp4', '.txt', '.html', '.doc', '.xls']
            for r in all_resources:
                res_html += f"<li><a href='{r['url']}' target='_blank'>{r['name']}</a></li>"
                if any(ext in r['url'].lower() for ext in exts):
                    download_file(r['url'], node_path, r['name'], session)
        res_html += "</ul>"
        
        save_html(node.get('title'), body_html, res_html, node_path / "content.html")
        
        vlink = meta.get('videoLink')
        if vlink: download_video(vlink, node_path / title)
        
        # Additional YouTube links from description
        y_links = re.findall(r'https://www\.(?:youtube\.com/watch\?v=|youtu\.be/)([\w-]+)', body_html)
        for i, yid in enumerate(set(y_links)):
            bonus_title = f"{title}_Bonus_{i+1}"
            download_video(f"https://www.youtube.com/watch?v={yid}", node_path / bonus_title)
            
    for child in node.get('children', []): process_node(child, node_path, session)

def downloader():
    flush_print("[START] RE-PARSING CONTENT...")
    config = get_config()
    output_base = Path(config.get("output_dir", "downloads"))
    session = get_requests_session()
    
    # Convert cookies for yt-dlp
    save_cookies_netscape(COOKIES_FILE, COOKIES_NETSCAPE)
    
    if not MAP_FILE.exists(): return flush_print("[ERR] map.json missing.")
    with open(MAP_FILE, 'r', encoding='utf-8') as f: data = json.load(f)
    for course in data.get("courses", []):
        cname = sanitize_filename(course.get('title', 'Course'))
        flush_print(f"\nðŸ“– [COURSE] {cname}")
        c_path = output_base / cname
        os.makedirs(c_path, exist_ok=True)
        for node in course.get('details', {}).get('hierarchy', []):
            process_node(node, c_path, session)
    flush_print("\nâœ… CONTENT RE-PARSE COMPLETE!")

if __name__ == "__main__":
    downloader()
